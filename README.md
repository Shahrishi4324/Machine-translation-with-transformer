# Machine Translation with Transformer from Scratch

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-1.7+-red.svg)

## Overview

This project implements a Transformer model from scratch using PyTorch for machine translation. The model is trained on the Multi30k dataset to translate sentences from German to English. The Transformer architecture, introduced by Vaswani et al. in 2017, is known for its self-attention mechanism and parallelizability, making it a powerful tool for sequence-to-sequence tasks.

## Dataset

The project uses the Multi30k dataset, a popular dataset for training machine translation models. It contains 30,000 parallel English-German sentence pairs.

## Features

- **Transformer Architecture:** Implementing a Transformer model from scratch, including encoder and decoder layers with multi-head attention.
- **Training on Multi30k Dataset:** Training the Transformer on a large dataset of English-German sentence pairs.
- **Sequence-to-Sequence Modeling:** Using the model for machine translation, translating German sentences to English.

## Installation

1. Clone the repository to your local machine:
   ```bash
   git clone https://github.com/Shahrishi4324/Machine-translation-with-transformer.git
   ```
2. Run the code:
   ```bash
   python Translationtransformer.py
   ```
